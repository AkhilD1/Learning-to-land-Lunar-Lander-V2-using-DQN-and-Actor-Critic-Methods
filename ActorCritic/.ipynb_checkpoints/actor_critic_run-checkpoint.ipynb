{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a449c93f-1706-4283-a76e-59273521aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c40e95-3946-49ef-b3c0-e32b4e1e1013",
   "metadata": {},
   "source": [
    "#### Create Lunar lander environment using gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8067937c-701c-406e-a0c9-006bc9a25e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in state space vector :  8\n",
      "Number of actions there :  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "number_of_actions = env.action_space.n\n",
    "print(\"Number of elements in state space vector : \", env.observation_space.shape[0])\n",
    "print(\"Number of actions there : \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a03f49a-2c4c-428b-b07b-db3325152102",
   "metadata": {},
   "source": [
    "#### We can consider that the Lunar Lander environment as solved when the average reward returns 200 over 100 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806a97c-7d1b-4cd0-b77e-c06cf98a3d33",
   "metadata": {},
   "source": [
    "#### Initialize constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd26e9b-809b-47dd-849f-fe81aac9a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "reward_threshold = 200\n",
    "# max number of iterations in training the model\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 1000\n",
    "min_episodes_criterion = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f164a8f3-0b58-4519-9d02-a673b0440668",
   "metadata": {},
   "source": [
    "#### We have implemented actor-critic in the ActorCrtic/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e70eb91-e043-4e74-9395-448615da8398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 03:22:53.014134: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-12-17 03:22:53.014509: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import and create Actor critic agent\n",
    "from main import Agent\n",
    "agent = Agent(env=env, gamma=0.99, learning_rate=0.01, num_hidden_units = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b4d9a85-4f4b-4bc2-a501-177ed51d2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(plot_results=True):\n",
    "    episodes_reward = []\n",
    "    average_rewards = []\n",
    "    reward_count = 0\n",
    "    with tqdm.trange(max_episodes) as t:\n",
    "        for i in t:\n",
    "            initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "            episode_reward = int(agent.train_step(initial_state))\n",
    "\n",
    "            episodes_reward.append(episode_reward)\n",
    "            running_reward = statistics.mean(episodes_reward)\n",
    "            average_reward = 0\n",
    "            if len(episodes_reward) > 100:\n",
    "                average_reward = np.mean(episodes_reward[-100:])\n",
    "                average_rewards.append(average_reward)\n",
    "\n",
    "            t.set_description(f'Episode {i}')\n",
    "            t.set_postfix(\n",
    "                episode_reward=episode_reward, average_reward=average_reward)\n",
    "\n",
    "            # Show average episode reward every 10 episodes\n",
    "            if i % 10 == 0:\n",
    "                pass  # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "            if average_reward >=200:\n",
    "                reward_count += 1\n",
    "            else:\n",
    "                reward_count = 0\n",
    "\n",
    "            if average_reward > reward_threshold and reward_count >= 200:\n",
    "                break\n",
    "\n",
    "    agent.model.save(\"actor_crtic_model\")\n",
    "    print(f'\\nSolved at episode {i}: average reward: {average_reward:.2f}!')\n",
    "    if plot_results:\n",
    "        plt.plot(episodes_reward, label=\"episode reward\")\n",
    "        plt.plot(average_rewards, label=\"average reward\")\n",
    "        plt.title(\"Actor-critic model\")\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3138cefd-36fa-437e-ba8b-adbbb4e47a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
    "    \n",
    "    scores, episodes, avg_scores, obj = [], [], [], []\n",
    "    goal = 200\n",
    "    score = 0.0\n",
    "    for i in range(max_steps):\n",
    "        state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "        done = False\n",
    "        episode_score = 0.0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            state = tf.expand_dims(state, 0)\n",
    "            action_probs, _ = model(state)\n",
    "            action = np.argmax(np.squeeze(action_probs))\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            state = tf.constant(state, dtype=tf.float32)\n",
    "            episode_score += reward\n",
    "            \n",
    "            # action = self.policy(state)\n",
    "            # new_state, reward, done, _ = env.step(action)\n",
    "            # episode_score += reward\n",
    "            # state = new_state\n",
    "        score += episode_score\n",
    "        print(episode_score)\n",
    "        scores.append(episode_score)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3084d78-a61c-4ade-8108-930cc4a5e144",
   "metadata": {},
   "source": [
    "#### For training the actor critic model set `train=True` in the below cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bafc4df-1f44-45bc-97db-2bd2b00ff313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 12:19:58.788 python[27574:3862834] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f8f4bd3b930>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-12-17 12:19:58.789 python[27574:3862834] Warning: Expected min height of view: (<NSButton: 0x7f8f4bd619a0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-12-17 12:19:58.791 python[27574:3862834] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f8f4bd58560>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-12-17 12:19:58.792 python[27574:3862834] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f8f2dc8de40>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282.10039579502165\n",
      "236.52912650427754\n",
      "284.43594088736523\n",
      "245.43029757135776\n",
      "269.617098720895\n",
      "260.7820034299867\n",
      "260.2576049762074\n",
      "255.57335408940557\n",
      "269.99097065768535\n",
      "271.3479720441195\n"
     ]
    }
   ],
   "source": [
    "train = False\n",
    "if train:\n",
    "    train_model()\n",
    "else:\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    env.reset()\n",
    "    # env = wrappers.Monitor(env, \"./actor_critic_results\", force=True)\n",
    "    model = tf.keras.models.load_model(\"actor_crtic_model\")\n",
    "    test(env, model, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca7b40-a0a2-451f-b718-05dce61987b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
